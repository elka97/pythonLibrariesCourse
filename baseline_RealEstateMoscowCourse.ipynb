{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RealEstateMoscowCourse.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjVoNYHp5LJBfSqQAFgEcA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elka97/pythonLibrariesCourse/blob/main/baseline_RealEstateMoscowCourse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTq7Gh8S8Ks3"
      },
      "source": [
        "Подключение библиотек и скриптов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaQeokEz6eaf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, StackingRegressor, VotingRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sklearn.metrics import r2_score as r2\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 12})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzEfcM488eON"
      },
      "source": [
        "    print(\"Train R2:\\t\" + str(round(r2(train_true_values, train_pred_values), 3)))\n",
        "    print(\"Test R2:\\t\" + str(round(r2(test_true_values, test_pred_values), 3)))\n",
        "    \n",
        "    plt.figure(figsize=(18,10))\n",
        "    \n",
        "    plt.subplot(121)\n",
        "    sns.scatterplot(x=train_pred_values, y=train_true_values)\n",
        "    plt.xlabel('Predicted values')\n",
        "    plt.ylabel('True values')\n",
        "    plt.title('Train sample prediction')\n",
        "    \n",
        "    plt.subplot(122)\n",
        "    sns.scatterplot(x=test_pred_values, y=test_true_values)\n",
        "    plt.xlabel('Predicted values')\n",
        "    plt.ylabel('True values')\n",
        "    plt.title('Test sample prediction')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex_GxoJX8fne"
      },
      "source": [
        "Пути к директориям и файлам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBqjdyb8hx_"
      },
      "source": [
        "TRAIN_DATASET_PATH = '/kaggle/input/real-estate-price-prediction-moscow/train.csv'\n",
        "TEST_DATASET_PATH = '/kaggle/input/real-estate-price-prediction-moscow/test.csv'\n",
        "\n",
        "TRAIN_DATASET_PATH = '/kaggle/input/real-estate-price-prediction-moscow/train.csv'\n",
        "TEST_DATASET_PATH = '/kaggle/input/real-estate-price-prediction-moscow/test.csv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_b_sqSY8r-e"
      },
      "source": [
        "Загрузка данных\n",
        "Описание датасета\n",
        "\n",
        "Id - идентификационный номер квартиры\n",
        "DistrictId - идентификационный номер района\n",
        "Rooms - количество комнат\n",
        "Square - площадь\n",
        "LifeSquare - жилая площадь\n",
        "KitchenSquare - площадь кухни\n",
        "Floor - этаж\n",
        "HouseFloor - количество этажей в доме\n",
        "HouseYear - год постройки дома\n",
        "Ecology_1, Ecology_2, Ecology_3 - экологические показатели местности\n",
        "Social_1, Social_2, Social_3 - социальные показатели местности\n",
        "Healthcare_1, Helthcare_2 - показатели местности, связанные с охраной здоровья\n",
        "Shops_1, Shops_2 - показатели, связанные с наличием магазинов, торговых центров\n",
        "Price - цена квартиры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csf_UqTd8u9H"
      },
      "source": [
        "train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
        "train_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wg1SQG28yeO"
      },
      "source": [
        "train_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNLT_DpX80jG"
      },
      "source": [
        "test_df = pd.read_csv(TEST_DATASET_PATH)\n",
        "test_df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8SLBqSk82wd"
      },
      "source": [
        "print('Строк в трейне:', train_df.shape[0])\n",
        "print('Строк в тесте', test_df.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnsF6CU084Ee"
      },
      "source": [
        "train_df.shape[1] - 1 == test_df.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1pby-zG87R-"
      },
      "source": [
        "Приведение типов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-wlWlzo88st"
      },
      "source": [
        "train_df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf9mHuzV8-Tu"
      },
      "source": [
        "train_df['Id'] = train_df['Id'].astype(str)\n",
        "train_df['DistrictId'] = train_df['DistrictId'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgJYX26V9Aj1"
      },
      "source": [
        "**# 1. EDA**\n",
        "\n",
        "Делаем EDA для:\n",
        "1.  Исправления выбросов \n",
        "2.  Заполнения NaN \n",
        "3.  Идей для генерации новых фич"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRL4Vwiu9RGu"
      },
      "source": [
        "Целевая переменная"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLQnrR5f9Wm1"
      },
      "source": [
        "plt.figure(figsize = (16, 8))\n",
        "\n",
        "train_df['Price'].hist(bins=30)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Price')\n",
        "\n",
        "plt.title('Target distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px8NQULw9a3W"
      },
      "source": [
        "Количественные переменные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcOaKtlQ9Z52"
      },
      "source": [
        "train_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6PHgR3_9fg-"
      },
      "source": [
        "Номинативные переменные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Hpj7KM9hC2"
      },
      "source": [
        "train_df.select_dtypes(include='object').columns.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw4CGXs79jDm"
      },
      "source": [
        "train_df['DistrictId'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un-lSqt09kp-"
      },
      "source": [
        "train_df['Ecology_2'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EvtSjzd9luY"
      },
      "source": [
        "train_df['Ecology_3'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjmCIXMI9nOG"
      },
      "source": [
        "train_df['Shops_2'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wdsLqUK98sw"
      },
      "source": [
        "**2. Обработка выбросов**\n",
        "Что можно делать с ними?\n",
        "\n",
        "Выкинуть эти данные (только на трейне, на тесте ничего не выкидываем)\n",
        "Заменять выбросы разными методами (медианы, средние значения, np.clip и т.д.)\n",
        "Делать/не делать дополнительную фичу\n",
        "Ничего не делать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NErdL0oR9_-4"
      },
      "source": [
        "Rooms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR1FIZ_5-BfX"
      },
      "source": [
        "train_df['Rooms'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh5TKKYw-DdP"
      },
      "source": [
        "train_df['Rooms_outlier'] = 0\n",
        "train_df.loc[(train_df['Rooms'] == 0) | (train_df['Rooms'] >= 6), 'Rooms_outlier'] = 1\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLRGJKlF-Gb7"
      },
      "source": [
        "train_df.loc[train_df['Rooms'] == 0, 'Rooms'] = 1\n",
        "train_df.loc[train_df['Rooms'] >= 6, 'Rooms'] = train_df['Rooms'].median()\n",
        "#train_df.loc[train_df['Rooms'] >= 6, 'Rooms'] = train_df['Rooms'].quantile(.975)\n",
        "#попробовать заменить на train_df['Rooms'].quantile(.975) для  >= 6 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCYjXliO-Hpm"
      },
      "source": [
        "train_df['Rooms'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLbVeHYI-JJe"
      },
      "source": [
        "KitchenSquare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suxfw-tn-Km_"
      },
      "source": [
        "train_df['KitchenSquare'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iavgP3ju-N31"
      },
      "source": [
        "train_df['KitchenSquare'].quantile(.975), train_df['KitchenSquare'].quantile(.025)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy_4aa1K-PzP"
      },
      "source": [
        "condition = (train_df['KitchenSquare'].isna()) \\\n",
        "             | (train_df['KitchenSquare'] > train_df['KitchenSquare'].quantile(.975))\n",
        "\n",
        "train_df['KitchenSquare_outlier'] = 0\n",
        "train_df.loc[condition | (train_df['KitchenSquare'] < 3), 'KitchenSquare_outlier'] = 1\n",
        "\n",
        "train_df.loc[condition, 'KitchenSquare'] = train_df['KitchenSquare'].median()\n",
        "train_df.loc[train_df['KitchenSquare'] < 3, 'KitchenSquare'] = 3\n",
        "# кухни, размер которых больше разницы между общей и жилой площадью - заменяем на разницу между общей и жилой площадью\n",
        "kitchenSquareCondition = (train_df['KitchenSquare'] > train_df['Square'] - train_df['LifeSquare']) & (train_df['Square'] > train_df['LifeSquare']) & (~train_df['LifeSquare'].isna())\n",
        "train_df.loc[kitchenSquareCondition, 'KitchenSquare_outlier'] = 1\n",
        "train_df.loc[kitchenSquareCondition, 'KitchenSquare'] = train_df['Square'] - train_df['LifeSquare']\n",
        "train_df['KitchenSquare'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndqFANdw-VhO"
      },
      "source": [
        "train_df['KitchenSquare'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIfLnbsl-XEG"
      },
      "source": [
        "HouseFloor, Floor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF4P4a83-YxX"
      },
      "source": [
        "train_df['HouseFloor'].sort_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iXpRDd9-aN2"
      },
      "source": [
        "train_df['Floor'].sort_values().unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YnKbjrF-be2"
      },
      "source": [
        "(train_df['Floor'] > train_df['HouseFloor']).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktj6E4IN-dLY"
      },
      "source": [
        "train_df['HouseFloor_outlier'] = 0\n",
        "train_df.loc[train_df['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\n",
        "train_df.loc[train_df['Floor'] > train_df['HouseFloor'], 'HouseFloor_outlier'] = 1\n",
        "train_df.loc[train_df['HouseFloor'] > train_df['HouseFloor'].quantile(.975), 'HouseFloor_outlier'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET4y6xQb-eou"
      },
      "source": [
        "train_df.loc[train_df['HouseFloor'] > train_df['HouseFloor'].quantile(.975), 'HouseFloor'] = train_df['HouseFloor'].quantile(.975)\n",
        "train_df.loc[train_df['HouseFloor'] == 0, 'HouseFloor'] = train_df['HouseFloor'].median()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlYi6D2_-gyO"
      },
      "source": [
        "floor_outliers = train_df.loc[train_df['Floor'] > train_df['HouseFloor']].index\n",
        "floor_outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPwsyBVb-iRG"
      },
      "source": [
        "train_df.loc[floor_outliers, 'Floor'] = train_df.loc[floor_outliers, 'HouseFloor']\\\n",
        "                                                .apply(lambda x: random.randint(1, x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIPEwy_p-kMG"
      },
      "source": [
        "(train_df['Floor'] > train_df['HouseFloor']).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuY30jay-mNG"
      },
      "source": [
        "HouseYear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z0XkJcU-oE2"
      },
      "source": [
        "train_df['HouseYear'].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njt6UetD-qTu"
      },
      "source": [
        "train_df.loc[train_df['HouseYear'] > 2021, 'HouseYear'] = 2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHOdoLQ-slm"
      },
      "source": [
        "3. Обработка пропусков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V0qwmuG-u3-"
      },
      "source": [
        "train_df.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUK0SY_C-xVe"
      },
      "source": [
        "train_df[['Square', 'LifeSquare', 'KitchenSquare']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eTl3pZU-z__"
      },
      "source": [
        "LifeSquare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_jZgt_C-3II"
      },
      "source": [
        "train_df['LifeSquare_nan'] = train_df['LifeSquare'].isna() * 1\n",
        "\n",
        "condition = (train_df['LifeSquare'].isna()) \\\n",
        "             & (~train_df['Square'].isna()) \\\n",
        "             & (~train_df['KitchenSquare'].isna())\n",
        "        \n",
        "train_df.loc[condition, 'LifeSquare'] = train_df.loc[condition, 'Square'] \\\n",
        "                                            - train_df.loc[condition, 'KitchenSquare'] - 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je-iQ4ux-5YO"
      },
      "source": [
        "Healthcare_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZwTS2Pm-6np"
      },
      "source": [
        "train_df.drop('Healthcare_1', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1yeKIun--iv"
      },
      "source": [
        "class DataPreprocessing:\n",
        "    \"\"\"Подготовка исходных данных\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Параметры класса\"\"\"\n",
        "        self.medians=None\n",
        "        self.kitchen_square_quantile = None\n",
        "        self.house_floor_quantile = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        \"\"\"Сохранение статистик\"\"\"       \n",
        "        # Расчет медиан\n",
        "        self.medians = X.median()\n",
        "        self.kitchen_square_quantile = X['KitchenSquare'].quantile(.975)\n",
        "        self.house_floor_quantile = X['HouseFloor'].quantile(.975)\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"Трансформация данных\"\"\"\n",
        "\n",
        "        # Rooms\n",
        "        X['Rooms_outlier'] = 0\n",
        "        X.loc[(X['Rooms'] == 0) | (X['Rooms'] >= 6), 'Rooms_outlier'] = 1\n",
        "        \n",
        "        ###X.loc[X['Rooms'] == 0, 'Rooms'] = 1\n",
        "        ###X.loc[X['Rooms'] >= 6, 'Rooms'] = self.medians['Rooms']\n",
        "        \n",
        "        info_by_district_id = X.groupby(['DistrictId', 'HouseYear'], as_index=False).agg({'Rooms': 'sum', 'Square': 'sum'}).rename(columns={'Rooms': 'sum_rooms_dr', 'Square': 'sum_square_dr'})\n",
        "\n",
        "        info_by_district_id['mean_square_per_room_in_dr'] = info_by_district_id['sum_square_dr'] / info_by_district_id['sum_rooms_dr']\n",
        "        info_by_district_id.drop(['sum_square_dr', 'sum_rooms_dr'], axis=1, inplace=True)\n",
        "\n",
        "        X = pd.merge(X, info_by_district_id, on=['DistrictId', 'HouseYear'], how='left')\n",
        "\n",
        "        X['mean_square_per_room_in_dr'] = X['mean_square_per_room_in_dr'].fillna(X['mean_square_per_room_in_dr'].mean())\n",
        "\n",
        "        X.loc[X['Rooms'] >= 6, 'Rooms'] = (X.loc[X['Rooms'] > 6, 'Square'] // X.loc[X['Rooms'] > 6, 'mean_square_per_room_in_dr']).astype('int')\n",
        "\n",
        "        X.loc[X['Rooms'] == 0, 'Rooms'] = (X.loc[X['Rooms'] == 0, 'Square'] // X.loc[X['Rooms'] == 0, 'mean_square_per_room_in_dr']).astype('int')        \n",
        "        \n",
        "        \n",
        "        # KitchenSquare\n",
        "        condition = (X['KitchenSquare'].isna()) \\\n",
        "                    | (X['KitchenSquare'] > self.kitchen_square_quantile)\n",
        "        \n",
        "        X.loc[condition, 'KitchenSquare'] = self.medians['KitchenSquare']\n",
        "\n",
        "        X.loc[X['KitchenSquare'] < 3, 'KitchenSquare'] = 3\n",
        "        \n",
        "        # Square\n",
        "        X.loc[X['Square'] > 400, 'Square'] = X.loc[X['Square'] > 400, 'Square'] / 10\n",
        "        \n",
        "        mu, sigma = norm.fit(X['Square'])\n",
        "\n",
        "        info_by_district_id = X.groupby(['DistrictId', 'Rooms', 'HouseYear'], as_index=False).agg({'Square': 'mean'}).rename(columns={'Square': 'mean_square_rooms_dr'})\n",
        "\n",
        "        X = pd.merge(X, info_by_district_id, on=['DistrictId', 'Rooms', 'HouseYear'], how='left')\n",
        "\n",
        "        X.loc[abs(X['Square'] - X['mean_square_rooms_dr']) > 2 * sigma, 'Square'] \\\n",
        "        = X.loc[abs(X['Square'] - X['mean_square_rooms_dr']) > 2 * sigma, 'Rooms'] \\\n",
        "        * X.loc[abs(X['Square'] - X['mean_square_rooms_dr']) > 2 * sigma, 'mean_square_per_room_in_dr']\n",
        "       \n",
        "        # кухни, размер которых больше разницы между общей и жилой площадью - заменяем на разницу между общей и жилой площадью\n",
        "        kitchenSquareCondition = (X['KitchenSquare'] > X['Square'] - X['LifeSquare']) & (X['Square'] > X['LifeSquare']) & (~X['LifeSquare'].isna())\n",
        "        X.loc[kitchenSquareCondition, 'KitchenSquare_outlier'] = 1\n",
        "        X.loc[kitchenSquareCondition, 'KitchenSquare'] = train_df['Square'] - train_df['LifeSquare']\n",
        "        \n",
        "        # HouseFloor, Floor\n",
        "        X['HouseFloor_outlier'] = 0\n",
        "        X.loc[X['HouseFloor'] == 0, 'HouseFloor_outlier'] = 1\n",
        "        X.loc[X['HouseFloor'] > self.house_floor_quantile, 'HouseFloor_outlier'] = 1\n",
        "        X.loc[X['Floor'] > X['HouseFloor'], 'HouseFloor_outlier'] = 1        \n",
        "        \n",
        "        X.loc[X['HouseFloor'] == 0, 'HouseFloor'] = self.medians['HouseFloor']\n",
        "        X.loc[X['HouseFloor'] > self.house_floor_quantile, 'HouseFloor'] = self.house_floor_quantile\n",
        "        \n",
        "        floor_outliers = X.loc[X['Floor'] > X['HouseFloor']].index\n",
        "        X.loc[floor_outliers, 'Floor'] = X.loc[floor_outliers, 'HouseFloor']\\\n",
        "                                            .apply(lambda x: random.randint(1, x))       \n",
        "\n",
        "        # HouseYear\n",
        "        current_year = datetime.now().year\n",
        "        \n",
        "        X['HouseYear_outlier'] = 0\n",
        "        X.loc[X['HouseYear'] > current_year, 'HouseYear_outlier'] = 1\n",
        "        \n",
        "        X.loc[X['HouseYear'] > current_year, 'HouseYear'] = current_year\n",
        "        \n",
        "        # Healthcare_1\n",
        "        if 'Healthcare_1' in X.columns:\n",
        "            X.drop('Healthcare_1', axis=1, inplace=True)\n",
        "            \n",
        "        # LifeSquare\n",
        "        X['LifeSquare_nan'] = X['LifeSquare'].isna() * 1\n",
        "        condition = (X['LifeSquare'].isna()) & \\\n",
        "                      (~X['Square'].isna()) & \\\n",
        "                      (~X['KitchenSquare'].isna())\n",
        "        \n",
        "        X.loc[condition, 'LifeSquare'] = X.loc[condition, 'Square'] - X.loc[condition, 'KitchenSquare'] - 3\n",
        "        \n",
        "        \n",
        "        X.fillna(self.medians, inplace=True)\n",
        "        \n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QqV0Wrs_Ckf"
      },
      "source": [
        "4. Построение новых признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCER9gkr_EoY"
      },
      "source": [
        "binary_to_numbers = {'A': 0, 'B': 1}\n",
        "\n",
        "train_df['Ecology_2'] = train_df['Ecology_2'].replace(binary_to_numbers)\n",
        "train_df['Ecology_3'] = train_df['Ecology_3'].replace(binary_to_numbers)\n",
        "train_df['Shops_2'] = train_df['Shops_2'].replace(binary_to_numbers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_wT1ES_Is0"
      },
      "source": [
        "DistrictSize, IsDistrictLarge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJQYqcyr_LHX"
      },
      "source": [
        "district_size = train_df['DistrictId'].value_counts().reset_index()\\\n",
        "                    .rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n",
        "\n",
        "district_size.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUZVUo38_RXe"
      },
      "source": [
        "train_df = train_df.merge(district_size, on='DistrictId', how='left')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtKGC8dS_S8v"
      },
      "source": [
        "(train_df['DistrictSize'] > 100).value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9dXFtbY_Uvo"
      },
      "source": [
        "train_df['IsDistrictLarge'] = (train_df['DistrictSize'] > 100).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBUi4XjO_Wzn"
      },
      "source": [
        "MedPriceByDistrict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sa4tBtQ_Y9A"
      },
      "source": [
        "med_price_by_district = train_df.groupby(['DistrictId', 'Rooms'], as_index=False).agg({'Price':'median'})\\\n",
        "                            .rename(columns={'Price':'MedPriceByDistrict'})\n",
        "\n",
        "med_price_by_district.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzWlaEE_gc9"
      },
      "source": [
        "train_df = train_df.merge(med_price_by_district, on=['DistrictId', 'Rooms'], how='left')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5VCCZ81_k2f"
      },
      "source": [
        "MedPriceByFloorYear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDqn0WJP_nUO"
      },
      "source": [
        "def floor_to_cat(X):\n",
        "\n",
        "    X['floor_cat'] = 0\n",
        "\n",
        "    X.loc[X['Floor'] <= 3, 'floor_cat'] = 1  \n",
        "    X.loc[(X['Floor'] > 3) & (X['Floor'] <= 5), 'floor_cat'] = 2\n",
        "    X.loc[(X['Floor'] > 5) & (X['Floor'] <= 9), 'floor_cat'] = 3\n",
        "    X.loc[(X['Floor'] > 9) & (X['Floor'] <= 15), 'floor_cat'] = 4\n",
        "    X.loc[X['Floor'] > 15, 'floor_cat'] = 5\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def floor_to_cat_pandas(X):\n",
        "    bins = [0, 3, 5, 9, 15, X['Floor'].max()]\n",
        "    X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n",
        "    \n",
        "    X['floor_cat'].fillna(-1, inplace=True)\n",
        "    return X\n",
        "\n",
        "\n",
        "def year_to_cat(X):\n",
        "\n",
        "    X['year_cat'] = 0\n",
        "\n",
        "    X.loc[X['HouseYear'] <= 1941, 'year_cat'] = 1\n",
        "    X.loc[(X['HouseYear'] > 1941) & (X['HouseYear'] <= 1945), 'year_cat'] = 2\n",
        "    X.loc[(X['HouseYear'] > 1945) & (X['HouseYear'] <= 1980), 'year_cat'] = 3\n",
        "    X.loc[(X['HouseYear'] > 1980) & (X['HouseYear'] <= 2000), 'year_cat'] = 4\n",
        "    X.loc[(X['HouseYear'] > 2000) & (X['HouseYear'] <= 2010), 'year_cat'] = 5\n",
        "    X.loc[(X['HouseYear'] > 2010), 'year_cat'] = 6\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def year_to_cat_pandas(X):\n",
        "    bins = [0, 1941, 1945, 1980, 2000, 2010, X['HouseYear'].max()]\n",
        "    X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n",
        "    \n",
        "    X['year_cat'].fillna(-1, inplace=True)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xogawjK5_wrn"
      },
      "source": [
        "bins = [0, 3, 5, 9, 15, train_df['Floor'].max()]\n",
        "pd.cut(train_df['Floor'], bins=bins, labels=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnsywFXX_z4P"
      },
      "source": [
        "bins = [0, 3, 5, 9, 15, train_df['Floor'].max()]\n",
        "pd.cut(train_df['Floor'], bins=bins)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2_zpSnq_1bn"
      },
      "source": [
        "train_df = year_to_cat(train_df)\n",
        "train_df = floor_to_cat(train_df)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aftvVKAO_3c3"
      },
      "source": [
        "med_price_by_floor_year = train_df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'Price':'median'}).\\\n",
        "                                            rename(columns={'Price':'MedPriceByFloorYear'})\n",
        "med_price_by_floor_year.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDtd8iWr_47x"
      },
      "source": [
        "train_df = train_df.merge(med_price_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__PZJR2x_82v"
      },
      "source": [
        "class FeatureGenetator():\n",
        "    \"\"\"Генерация новых фич\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.DistrictId_counts = None\n",
        "        self.binary_to_numbers = None\n",
        "        self.med_price_by_district = None\n",
        "        self.med_price_by_floor_year = None\n",
        "        self.house_year_max = None\n",
        "        self.floor_max = None\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        \n",
        "        X = X.copy()\n",
        "        \n",
        "        # Binary features\n",
        "        self.binary_to_numbers = {'A': 0, 'B': 1}\n",
        "        \n",
        "        # DistrictID\n",
        "        self.district_size = X['DistrictId'].value_counts().reset_index() \\\n",
        "                               .rename(columns={'index':'DistrictId', 'DistrictId':'DistrictSize'})\n",
        "                \n",
        "        # Target encoding\n",
        "        ## District, Rooms\n",
        "        df = X.copy()\n",
        "        \n",
        "        if y is not None:\n",
        "            df['Price'] = y.values\n",
        "            \n",
        "            self.med_price_by_district = df.groupby(['DistrictId', 'Rooms'], as_index=False).agg({'Price':'median'})\\\n",
        "                                            .rename(columns={'Price':'MedPriceByDistrict'})\n",
        "            \n",
        "            self.med_price_by_district_median = self.med_price_by_district['MedPriceByDistrict'].median()\n",
        "            \n",
        "        ## floor, year\n",
        "        if y is not None:\n",
        "            self.floor_max = df['Floor'].max()\n",
        "            self.house_year_max = df['HouseYear'].max()\n",
        "            df['Price'] = y.values\n",
        "            df = self.floor_to_cat(df)\n",
        "            df = self.year_to_cat(df)\n",
        "            self.med_price_by_floor_year = df.groupby(['year_cat', 'floor_cat'], as_index=False).agg({'Price':'median'}).\\\n",
        "                                            rename(columns={'Price':'MedPriceByFloorYear'})\n",
        "            self.med_price_by_floor_year_median = self.med_price_by_floor_year['MedPriceByFloorYear'].median()\n",
        "        \n",
        "\n",
        "        \n",
        "    def transform(self, X):\n",
        "        \n",
        "        # Binary features\n",
        "        X['Ecology_2'] = X['Ecology_2'].map(self.binary_to_numbers)  # self.binary_to_numbers = {'A': 0, 'B': 1}\n",
        "        X['Ecology_3'] = X['Ecology_3'].map(self.binary_to_numbers)\n",
        "        X['Shops_2'] = X['Shops_2'].map(self.binary_to_numbers)\n",
        "        \n",
        "        # DistrictId, IsDistrictLarge\n",
        "        X = X.merge(self.district_size, on='DistrictId', how='left')\n",
        "        \n",
        "        X['new_district'] = 0\n",
        "        X.loc[X['DistrictSize'].isna(), 'new_district'] = 1\n",
        "\n",
        "        X['First_Floor'] = 0\n",
        "        X.loc[X['Floor'] == 1, 'First_Floor'] = 1\n",
        "        X['Last_Floor'] = 0\n",
        "        X.loc[X['Floor'] == X['HouseFloor'], 'Last_Floor'] = 1\n",
        "        \n",
        "        X['DistrictSize'].fillna(5, inplace=True)\n",
        "        \n",
        "        X['IsDistrictLarge'] = (X['DistrictSize'] > 100).astype(int)\n",
        "        \n",
        "        # More categorical features\n",
        "        X = self.floor_to_cat(X)  # + столбец floor_cat\n",
        "        X = self.year_to_cat(X)   # + столбец year_cat\n",
        "        \n",
        "        # Target encoding\n",
        "        if self.med_price_by_district is not None:\n",
        "            X = X.merge(self.med_price_by_district, on=['DistrictId', 'Rooms'], how='left')\n",
        "            X.fillna(self.med_price_by_district_median, inplace=True)\n",
        "            \n",
        "        if self.med_price_by_floor_year is not None:\n",
        "            X = X.merge(self.med_price_by_floor_year, on=['year_cat', 'floor_cat'], how='left')\n",
        "            X.fillna(self.med_price_by_floor_year_median, inplace=True)\n",
        "        \n",
        "        return X\n",
        "    \n",
        "    def floor_to_cat(self, X):\n",
        "        bins = [0, 3, 5, 9, 15, self.floor_max]\n",
        "        X['floor_cat'] = pd.cut(X['Floor'], bins=bins, labels=False)\n",
        "\n",
        "        X['floor_cat'].fillna(-1, inplace=True) \n",
        "        return X\n",
        "     \n",
        "    def year_to_cat(self, X):\n",
        "        bins = [0, 1941, 1945, 1980, 2000, 2010, self.house_year_max]\n",
        "        X['year_cat'] = pd.cut(X['HouseYear'], bins=bins, labels=False)\n",
        "\n",
        "        X['year_cat'].fillna(-1, inplace=True)\n",
        "        return X           \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO9GleHJAHer"
      },
      "source": [
        "5. Отбор признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em1mZmtaAIyA"
      },
      "source": [
        "train_df.columns.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SW4xFoBALWZ"
      },
      "source": [
        "feature_names = ['Rooms', 'Square', 'LifeSquare', 'KitchenSquare', 'Floor', 'HouseFloor', 'HouseYear',\n",
        "                 'Ecology_1', 'Ecology_2', 'Ecology_3', 'Social_1', 'Social_2', 'Social_3',\n",
        "                 'Helthcare_2', 'Shops_1']\n",
        "\n",
        "new_feature_names = ['Rooms_outlier', 'HouseFloor_outlier', 'HouseYear_outlier', 'LifeSquare_nan', 'DistrictSize',\n",
        "                     'new_district', 'IsDistrictLarge',  'MedPriceByFloorYear', 'First_Floor', 'Last_Floor']\n",
        "\n",
        "target_name = 'Price'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo9rBpGnAP2J"
      },
      "source": [
        "Normal distribution of the target variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LOz1DOaATY6"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "sns.distplot(train_df['Price'], fit=norm)\n",
        "\n",
        "mu, sigma = norm.fit(train_df['Price'])\n",
        "\n",
        "print(f'mu = {mu:.2f} and sigma = {sigma:.2f}')\n",
        "\n",
        "plt.legend(\n",
        "    [f'Normal dist. ($\\mu=$ {mu:.2f} and $\\sigma=$ {sigma:.2f} )'], loc='best')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Price distribution')\n",
        "\n",
        "# QQ-plot\n",
        "fig = plt.figure()\n",
        "res = stats.probplot(train_df['Price'], plot=plt)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avZ8Q3w5AcIe"
      },
      "source": [
        "Data Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6HFT4dyAec5"
      },
      "source": [
        "corrmat = train_df.loc[:, train_df.columns != 'Id'].corr()\n",
        "plt.subplots(figsize=(12, 9))\n",
        "sns.heatmap(corrmat, vmax=0.9, square=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8yecNGuAgtY"
      },
      "source": [
        "corrmat = train_df.loc[:, train_df.columns != 'Id'].corrwith(\n",
        "    train_df['Price']).abs().sort_values(ascending=False)[1:]\n",
        "plt.bar(corrmat.index, corrmat.values)\n",
        "plt.title('Correlation to Price')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmGCpFaZAi3Z"
      },
      "source": [
        "6. Разбиение на train и test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVw_OuHkAkHA"
      },
      "source": [
        "train_df = pd.read_csv(TRAIN_DATASET_PATH)\n",
        "test_df = pd.read_csv(TEST_DATASET_PATH)\n",
        "\n",
        "X = train_df.drop(columns=target_name)\n",
        "y = train_df[target_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zls30OvaAlg6"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMzI7cN3AnD4"
      },
      "source": [
        "preprocessor = DataPreprocessing()\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "X_train = preprocessor.transform(X_train)\n",
        "X_valid = preprocessor.transform(X_valid)\n",
        "test_df = preprocessor.transform(test_df)\n",
        "\n",
        "X_train.shape, X_valid.shape, test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_D2ki3eApAS"
      },
      "source": [
        "features_gen = FeatureGenetator()\n",
        "features_gen.fit(X_train, y_train)\n",
        "\n",
        "X_train = features_gen.transform(X_train)\n",
        "X_valid = features_gen.transform(X_valid)\n",
        "test_df = features_gen.transform(test_df)\n",
        "\n",
        "X_train.shape, X_valid.shape, test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3Oi6XTNAqi4"
      },
      "source": [
        "X_train = X_train[feature_names + new_feature_names]\n",
        "X_valid = X_valid[feature_names + new_feature_names]\n",
        "test_df = test_df[feature_names + new_feature_names]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh0FfDWRAtJi"
      },
      "source": [
        "X_train.isna().sum().sum(), X_valid.isna().sum().sum(), test_df.isna().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk8_lq2nAu5Q"
      },
      "source": [
        "7. Построение модели"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htv1QxWFAwOi"
      },
      "source": [
        "Подбор оптимальной модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HJO8heqAyUg"
      },
      "source": [
        "#from sklearn.model_selection import RandomizedSearchCV\n",
        "#from pprint import pprint\n",
        "\n",
        "## Number of trees in random forest\n",
        "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "## Number of features to consider at every split\n",
        "#max_features = ['auto', 'sqrt']\n",
        "## Maximum number of levels in tree\n",
        "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "#max_depth.append(None)\n",
        "## Minimum number of samples required to split a node\n",
        "#min_samples_split = [2, 5, 10]\n",
        "## Minimum number of samples required at each leaf node\n",
        "#min_samples_leaf = [1, 2, 4]\n",
        "## Method of selecting samples for training each tree\n",
        "#bootstrap = [True, False]\n",
        "## Create the random grid\n",
        "#random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
        "#                'importance_type':['weight', 'gain', 'cover', 'total_gain', 'total_cover'],\n",
        "#                'booster':['gbtree', 'gblinear', 'dart'],\n",
        "#                'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
        "#                'tree_method':['auto', 'exact', 'approx', 'hist', 'gpu_hist'],\n",
        "#                'subsample': [0.9, 1.0],\n",
        "#                'colsample_bytree': [0.9, 1.0]}\n",
        "           \n",
        "#pprint(random_grid)\n",
        "\n",
        "### Use the random grid to search for best hyperparameters\n",
        "## First create the base model to tune\n",
        "#rf = XGBRegressor(random_state=21)\n",
        "## Random search of parameters, using 3 fold cross validation, \n",
        "## search across 100 different combinations, and use all available cores\n",
        "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=21, n_jobs = -1)\n",
        "## Fit the random search model\n",
        "#rf_random.fit(X_train, y_train)\n",
        "#rf_random.best_params_\n",
        "\n",
        "#XGBRegressor(random_state=21,\n",
        "#               tree_method='hist',\n",
        "#               subsample=1.0,\n",
        "#               n_estimators=600,\n",
        "#               max_depth=3,\n",
        "#               importance_type='total_cover',\n",
        "#               colsample_bytree=1.0,\n",
        "#               booster='dart')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9DrRoXUA12o"
      },
      "source": [
        "#from sklearn.model_selection import RandomizedSearchCV\n",
        "#from pprint import pprint\n",
        "\n",
        "## Number of trees in random forest\n",
        "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "## Number of features to consider at every split\n",
        "#max_features = ['auto', 'sqrt']\n",
        "## Maximum number of levels in tree\n",
        "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "#max_depth.append(None)\n",
        "## Minimum number of samples required to split a node\n",
        "#min_samples_split = [2, 5, 10]\n",
        "## Minimum number of samples required at each leaf node\n",
        "#min_samples_leaf = [1, 2, 4]\n",
        "## Method of selecting samples for training each tree\n",
        "#bootstrap = [True, False]\n",
        "## Create the random grid\n",
        "#random_grid = {'n_estimators': n_estimators,\n",
        "#               'max_features': max_features,\n",
        "#               'max_depth': max_depth,\n",
        "#               'min_samples_split': min_samples_split,\n",
        "#               'min_samples_leaf': min_samples_leaf,\n",
        "#               'bootstrap': bootstrap}\n",
        "#pprint(random_grid)\n",
        "\n",
        "\n",
        "### Use the random grid to search for best hyperparameters\n",
        "## First create the base model to tune\n",
        "#rf = RandomForestRegressor(random_state=21, criterion='mse')\n",
        "## Random search of parameters, using 3 fold cross validation, \n",
        "## search across 100 different combinations, and use all available cores\n",
        "#rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=21, n_jobs = -1)\n",
        "## Fit the random search model\n",
        "#rf_random.fit(X_train, y_train)\n",
        "#rf_random.best_params_\n",
        "\n",
        "#(random_state=21,\n",
        "# criterion='mse',\n",
        "# n_estimators=1800,\n",
        "# min_samples_split=2,\n",
        "# min_samples_leaf=2,\n",
        "# max_features='sqrt',\n",
        "# max_depth=80,\n",
        "# bootstrap=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDfbliVeA6QR"
      },
      "source": [
        "#from pprint import pprint\n",
        "#from lightgbm import LGBMRegressor\n",
        "\n",
        "## Create the random grid\n",
        "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "#max_bin  = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "#random_grid = {\n",
        "#    'num_leaves': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "#    'max_depth': [1, 2, 3, 4, 5],\n",
        "#    'max_bin': max_bin,\n",
        "#    'n_estimators': np.arange(1000, 7000, 1000),\n",
        "#    'learning_rate': np.arange(0.01, 0.05, 0.01)\n",
        "#}\n",
        "#pprint(random_grid)\n",
        "\n",
        "#### Use the random grid to search for best hyperparameters\n",
        "### First create the base model to tune\n",
        "#lgbm = LGBMRegressor(random_state=21)\n",
        "### Random search of parameters, using 3 fold cross validation, \n",
        "### search across 100 different combinations, and use all available cores\n",
        "#lgbm_random = RandomizedSearchCV(estimator = lgbm, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=21, n_jobs = -1)\n",
        "## Fit the random search model\n",
        "#lgbm_random.fit(X_train, y_train)\n",
        "\n",
        "#print(lgbm_random.best_params_)\n",
        "\n",
        "\n",
        "####LGBMRegressor(random_state=21, \n",
        "####              num_leaves=10,\n",
        "####              n_estimators=1000,\n",
        "####              max_depth=5,\n",
        "####              max_bin=30,\n",
        "####              learning_rate= 0.03)\n",
        "\n",
        "####rf_model = LGBMRegressor(random_state=21, \n",
        "####                         num_leaves=5,\n",
        "####                         n_estimators=3000,\n",
        "####                         max_depth=5,\n",
        "####                         max_bin=80,\n",
        "####                         learning_rate= 0.03)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w4QjzYoA8T4"
      },
      "source": [
        "#from sklearn.model_selection import RandomizedSearchCV\n",
        "#from pprint import pprint\n",
        "## Number of trees in random forest\n",
        "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "## Number of features to consider at every split\n",
        "#max_features = ['auto', 'sqrt', 'log2']\n",
        "## Maximum number of levels in tree\n",
        "#max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "#max_depth.append(None)\n",
        "## Minimum number of samples required to split a node\n",
        "#min_samples_split = [2, 5, 8, 10, 12]\n",
        "## Minimum number of samples required at each leaf node\n",
        "#min_samples_leaf = [1, 2, 3, 4, 5]\n",
        "## The function to measure the quality of a split\n",
        "#criterion = ['mse']\n",
        "## Create the random grid\n",
        "#random_grid = {'criterion': criterion,\n",
        "#               'max_depth': max_depth,\n",
        "#               'max_features': max_features,\n",
        "#               'min_samples_leaf': min_samples_leaf,\n",
        "#               'min_samples_split': min_samples_split,\n",
        "#               'n_estimators': np.arange(1000, 7000, 1000),}\n",
        "#pprint(random_grid)\n",
        "\n",
        "\n",
        "### Use the random grid to search for best hyperparameters\n",
        "## First create the base model to tune\n",
        "#gb = GradientBoostingRegressor(random_state=21)\n",
        "## Random search of parameters, using 3 fold cross validation, \n",
        "## search across 100 different combinations, and use all available cores\n",
        "#gb_random = RandomizedSearchCV(estimator = gb, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=21, n_jobs = -1)\n",
        "## Fit the random search model\n",
        "#gb_random.fit(X_train, y_train)\n",
        "#gb_random.best_params_\n",
        "\n",
        "\n",
        "#{'n_estimators': 3000,\n",
        "# 'min_samples_split': 12,\n",
        "# 'min_samples_leaf': 5,\n",
        "# 'max_features': 'sqrt',\n",
        "# 'max_depth': 40,\n",
        "# 'criterion': 'mse'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pGz3jMzA_zB"
      },
      "source": [
        "####r = RandomForestRegressor(random_state=21,\n",
        "####                                 criterion='mse',\n",
        "####                                 n_estimators=1800,\n",
        "####                                 min_samples_split=2,\n",
        "####                                 min_samples_leaf=2,\n",
        "####                                 max_features='sqrt',\n",
        "####                                 max_depth=80,\n",
        "####                                 bootstrap=False)\n",
        "\n",
        "####gb = GradientBoostingRegressor(random_state=21,\n",
        "####                                    n_estimators=3000,\n",
        "####                                    min_samples_split=12,\n",
        "####                                    min_samples_leaf=5,\n",
        "####                                    max_features='sqrt',\n",
        "####                                    max_depth=40,\n",
        "####                                    criterion='mse')\n",
        "\n",
        "####lgb = LGBMRegressor(random_state=21, \n",
        "####              num_leaves=10,\n",
        "####              n_estimators=1000,\n",
        "####              max_depth=5,\n",
        "####              max_bin=30,\n",
        "####              learning_rate= 0.03)\n",
        "\n",
        "####xgb = XGBRegressor(random_state=21,\n",
        "####                       tree_method='hist',\n",
        "####                       subsample=1.0,\n",
        "####                       n_estimators=600,\n",
        "####                       max_depth=3,\n",
        "####                       importance_type='total_cover',\n",
        "####                       colsample_bytree=1.0,\n",
        "####                       booster='dart')\n",
        "####er_model = VotingRegressor([('r', r), ('gb', gb), ('lgb', lgb), ('xgb', xgb)])\n",
        "####er_model.fit(X_train, y_train)\n",
        "\n",
        "####y_train_preds = er_model.predict(X_train)\n",
        "####y_test_preds = er_model.predict(X_valid)\n",
        "\n",
        "####evaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)\n",
        "####from sklearn.ensemble import StackingRegressor, VotingRegressor, BaggingRegressor, GradientBoostingRegressor\n",
        "\n",
        "####r = RandomForestRegressor(random_state=21,\n",
        "####                                 criterion='mse',\n",
        "####                                 n_estimators=1800,\n",
        "####                                 min_samples_split=2,\n",
        "####                                 min_samples_leaf=2,\n",
        "####                                 max_features='sqrt',\n",
        "####                                 max_depth=80,\n",
        "####                                 bootstrap=False)\n",
        "\n",
        "####gb = GradientBoostingRegressor(random_state=21,\n",
        "####                                    n_estimators=3000,\n",
        "####                                    min_samples_split=12,\n",
        "####                                    min_samples_leaf=5,\n",
        "####                                    max_features='sqrt',\n",
        "####                                    max_depth=40,\n",
        "####                                    criterion='mse')\n",
        "\n",
        "####lgb = LGBMRegressor(random_state=21, \n",
        "####              num_leaves=10,\n",
        "####              n_estimators=1000,\n",
        "####              max_depth=5,\n",
        "####              max_bin=30,\n",
        "####              learning_rate= 0.03)\n",
        "\n",
        "####xgb = XGBRegressor(random_state=21,\n",
        "####                       tree_method='hist',\n",
        "####                       subsample=1.0,\n",
        "####                       n_estimators=600,\n",
        "####                       max_depth=3,\n",
        "####                       importance_type='total_cover',\n",
        "####                       colsample_bytree=1.0,\n",
        "####                       booster='dart')\n",
        "\n",
        "####stack = StackingRegressor([('r',r), ('gb',gb), ('xgb',xgb), ('lbg', lgb))\n",
        "####stack.fit(X_train, y_train)\n",
        "\n",
        "####y_train_preds = er_model.predict(X_train)\n",
        "####y_test_preds = er_model.predict(X_valid)\n",
        "\n",
        "####evaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)\n",
        "####bg_model = BaggingRegressor(base_estimator=rf_model,  n_estimators=8, random_state=0)\n",
        "####bg_model.fit(X_train, y_train)\n",
        "\n",
        "####y_train_preds = bg_model.predict(X_train)\n",
        "####y_test_preds = bg_model.predict(X_valid)\n",
        "\n",
        "####evaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQWtIxenBBxB"
      },
      "source": [
        "#rf_model = RandomForestRegressor(random_state=21,\n",
        "#                                 criterion='mse',\n",
        "#                                 n_estimators=4000,\n",
        "#                                 min_samples_split=4,\n",
        "#                                 min_samples_leaf=1,\n",
        "#                                 max_features='sqrt',\n",
        "#                                 max_depth=100,\n",
        "#                                 bootstrap=False)\n",
        "#rf_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McT2LmPnBDLR"
      },
      "source": [
        "**Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O38SWprmBEN-"
      },
      "source": [
        "lgbm_random = LGBMRegressor(random_state=21, \n",
        "              n_estimators=4000,\n",
        "              max_depth=8,\n",
        "              max_bin=150,\n",
        "              num_leaves=10,\n",
        "              learning_rate=0.01)\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=21,\n",
        "                                 criterion='mse',\n",
        "                                 n_estimators=4000,\n",
        "                                 min_samples_split=4,\n",
        "                                 min_samples_leaf=1,\n",
        "                                 max_features='sqrt',\n",
        "                                 max_depth=100,\n",
        "                                 bootstrap=False)\n",
        "\n",
        "stack = StackingRegressor([('lgb',lgbm_random), ('rf',rf_model)])\n",
        "stack.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hykd-X__BI2I"
      },
      "source": [
        "**Оценка модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2NMgUpEBK82"
      },
      "source": [
        "y_train_preds = stack.predict(X_train)\n",
        "y_test_preds = stack.predict(X_valid)\n",
        "\n",
        "evaluate_preds(y_train, y_train_preds, y_valid, y_test_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkLfUZbQBP64"
      },
      "source": [
        "Кросс-валидация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqfemZjBRIQ"
      },
      "source": [
        "cv_score = cross_val_score(stack, X_train, y_train, scoring='r2', cv=KFold(n_splits=3, shuffle=True, random_state=21))\n",
        "cv_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc60OECLBWjT"
      },
      "source": [
        "cv_score.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmJVjgpeBX1Y"
      },
      "source": [
        "Важность признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4Yub1PsBZTw"
      },
      "source": [
        "#feature_importances = pd.DataFrame(zip(X_train.columns, stack.feature_importances_), \n",
        "#                                    columns=['feature_name', 'importance'])\n",
        "\n",
        "#feature_importances.sort_values(by='importance', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ7o27wtBayZ"
      },
      "source": [
        "8. Прогнозирование на тестовом датасете\n",
        "Выполнить для тестового датасета те же этапы обработки и постронияния признаков\n",
        "Не потерять и не перемешать индексы от примеров при построении прогнозов\n",
        "Прогнозы должны быть для все примеров из тестового датасета (для всех строк)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hTl8W7UBdTL"
      },
      "source": [
        "test_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vst17YR3Be4x"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ6i4CLCBgRJ"
      },
      "source": [
        "submit = pd.read_csv('/kaggle/input/real-estate-price-prediction-moscow/sample_submission.csv')\n",
        "submit.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86_2fADpBjWh"
      },
      "source": [
        "predictions = stack.predict(test_df)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNegnuJTBloT"
      },
      "source": [
        "submit['Price'] = predictions\n",
        "submit.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XhaOW2QBnly"
      },
      "source": [
        "submit.to_csv('rf_submit.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}